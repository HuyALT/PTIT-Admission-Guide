{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil\n",
        "\n",
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "else:\n",
        "    print(\"GPU not available.\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "h1cPkEuA8EJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "xFMNNDrq--9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "I3LXD7y4BtDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                            quantization_config=bnb_config)"
      ],
      "metadata": {
        "id": "052aP0TUCIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"csv\", data_files=\"dataset.csv\", split=\"train\")\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QyIB6oYGCJM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "                                           add_eos_token=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
      ],
      "metadata": {
        "id": "mmsFcTZdDkvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'],padding=\"max_length\", truncation=True)\n",
        "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "TwNKhaB9DonH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    # rank of the update matrices\n",
        "    # Lower rank results in smaller matrices with fewer trainable params\n",
        "    r=8,\n",
        "\n",
        "    # impacts low-rank approximation aggressiveness\n",
        "    # increasing value speeds up training\n",
        "    lora_alpha=64,\n",
        "\n",
        "    # modules to apply the LoRA update matrices\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"down_proj\",\n",
        "        \"up_proj\",\n",
        "        \"o_proj\"\n",
        "    ],\n",
        "\n",
        "    # determines LoRA bias type, influencing training dynamics\n",
        "    bias=\"none\",\n",
        "\n",
        "    # regulates model regularization; increasing may lead to underfitting\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "k_3JR5nhGJCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model, # Mô hình sử dụng tinh chỉnh\n",
        "    train_dataset=tokenized_dataset, # Bộ dataset sử dụng tinh chỉnh\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"./finetuned-llama-2-model-TuyenSinhPTIT2024\",  # Thư mục lưu lại các checkpoint huấn luyện\n",
        "        per_device_train_batch_size=2, # Số mẫu dữ liệu mỗi bath cho GPU\n",
        "        gradient_accumulation_steps=2, # Tích lũy gradient sau 2 bước trước khi thực hiện cập nhật trọng số\n",
        "        num_train_epochs=3, # Số lần duyệt qua toàn bộ dữ liệu huấn luyện\n",
        "        learning_rate=2e-4, # Xác định mức độ thay đổi trọng số trong mỗi bước cập nhật\n",
        "        bf16=False,      # Tắt định dạng bfloat16 do phần cứng không hỗ trợ\n",
        "        optim=\"paged_adamw_8bit\", # Sử dụng tối ưu hóa Adamw phiên bản 8bit để tiết kiệm bộ nhớ và tăng hiệu suất\n",
        "        logging_dir=\"./logs\",# Thư mục sử dụng để lưu thông tin huấn luyện\n",
        "        save_strategy=\"epoch\", # Lưu lại model huấn luyện sau mỗi epoch\n",
        "        logging_steps = 10, # Ghi lại thông tin nhật kí huấn luyện\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False # Tắt cache trong bộ huấn luyện để tối ưu bộ nhớ\n",
        "trainer.train() # Tiến hành huấn luyện\n",
        "\n",
        "trainer.save_model(\"./finetuned_llama2_7b_chat\")\n",
        "tokenizer.save_pretrained(\"./finetuned_llama2_7b_chat\")"
      ],
      "metadata": {
        "id": "v5ENwT1vGTT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "TECo7IHBGwn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "nf4Config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "                                           trust_remote_code=True)\n",
        "\n",
        "\n",
        "# modelFinetuned = PeftModel.from_pretrained(base_model,\"finetuneModel\")\n",
        "# modelFinetuned = modelFinetuned.merge_and_unload()"
      ],
      "metadata": {
        "id": "OOvSK5Q8Gvp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ENTER YOUR QUESTION BELOW\n",
        "\n",
        "question = \"Trả lời câu hỏi sau bằng tiếng việt: Điểm chuẩn ngành công nghệ thông tin của Học viện công nghệ bưu chính viễn thông năm 2023 tại cơ sở BVH?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"{question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kh2z8qfYHKYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelFinetuned = PeftModel.from_pretrained(base_model,\"newmodel\")\n",
        "modelFinetuned = modelFinetuned.merge_and_unload()"
      ],
      "metadata": {
        "id": "q9Do0Lxh5aSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelFinetuned.save_pretrained(\"./llama-2-chat-hf-for-TuyenSinhPTIT2024\")"
      ],
      "metadata": {
        "id": "hZU0u2-sVedQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelFinetuned.push_to_hub(\"llama-2-chat-hf-for-TuyenSinhPTIT2024\", use_temp_dir=False)\n",
        "tokenizer.push_to_hub(\"llama-2-chat-hf-for-TuyenSinhPTIT2024\", use_temp_dir=False)"
      ],
      "metadata": {
        "id": "QQvLo_FrXECZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}